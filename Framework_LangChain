pip install -U langchain-community
pip install pypdf
pip install tiktoken

# Alternatively, CPU can also be replaced by GPU.
# The effects of switching to GPU are not yet known.
pip install faiss-cpu

# to ensure that the latest version of OpenAI is installed.
!pip install --upgrade openai

import openai
from langchain.vectorstores import FAISS
from langchain.embeddings import OpenAIEmbeddings

# set OpenAI API key
# Open API key might not be needed here, as it is set in line 21 (needs to be verified)
OPENAI_API_KEY = "API_Key"

import os
os.environ["OPENAI_API_KEY"] = "API_Key"

from langchain.document_loaders import PyPDFLoader
# Load each PDF file individually and combine the documents
documents = []
# replace "/path/to/your/directory/" with your local path file or type in the file names you have uploaded in Google Colab
for file_path in ["/path/to/your/directory/"]:
    loader = PyPDFLoader(file_path)  # Create a new loader for each file
    documents.extend(loader.load())  # Extend the documents list with loaded documents

from langchain.text_splitter import RecursiveCharacterTextSplitter
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
chunks = text_splitter.split_documents(documents)

#might have to change the embedding model
#Output how much of the text could be embedded or similar to assess the success of the embedding (at a later stage)
embeddings = OpenAIEmbeddings(model="text-embedding-ada-002")

#ensures that the chunks are converted into vectors (must be done for each new document)
faiss_index = FAISS.from_documents(chunks, embeddings)
#stores the previously computed or created vectors to avoid recalculating or recreating them
faiss_index.save_local("faiss_index")

# 1. Retrieval of relevant information from FAISS
#vector_store = embeddings.embed_documents([chunk.page_content for chunk in chunks]) macht das gleiche wie nachfolgende Zeile
vectorstore = FAISS.load_local("faiss_index", embeddings, allow_dangerous_deserialization=True)
retriever = vectorstore.as_retriever()

#Enter User Input here
query = "Enter User Insput here"
retrieved_docs = retriever.get_relevant_documents(query)

# Extract the content of the documents
retrieved_content = "\n".join([doc.page_content for doc in retrieved_docs])

# 2. Create the prompt for ChatGPT
prompt = f"""
Here are some relevant pieces of information from the knowledge base:
{retrieved_content}

Use this information to answer the following question:
{query}
"""

# 3. Send prompt to OpenAI-API
response =  openai.chat.completions.create(
    # The model can be adjusted if desired (see the following link for available models)
    # https://platform.openai.com/docs/models
    model="gpt-4o-mini",
    messages=[{"role": "system", "content": "Du bist ein hilfreicher Assistent."},
              {"role": "user", "content": prompt}]
)

# Print the response 
print(response.choices[0].message.content)
